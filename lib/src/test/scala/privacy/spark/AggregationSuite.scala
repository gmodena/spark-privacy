/*
 * This Scala Testsuite was generated by the Gradle 'init' task.
 */
package privacy.spark

import org.scalatest.funsuite.AnyFunSuite
import org.junit.runner.RunWith
import org.scalatestplus.junit.JUnitRunner
import com.holdenkarau.spark.testing.DataFrameSuiteBase
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row, functions}
import org.apache.spark.sql.types.{StringType, StructField, StructType}

case class Visitors(schema: StructType, data: Seq[Row])

@RunWith(classOf[JUnitRunner])
class AggregationSuiteCheck extends AnyFunSuite with DataFrameSuiteBase {
  val visitors = Visitors(
    schema = StructType(List(StructField("VisitorId", StringType, nullable = true),
      StructField("TimeEntered", StringType, nullable = true),
      StructField("TimeSpent", StringType, nullable = true),
      StructField("MoneySpent", StringType, nullable = true),
      StructField("Day", StringType, nullable = true))),
    data = List(Row("1", "9:30:00 AM", "26", "24", "1"),
      Row("2", "10d:30:00 AM", "10", "1", "1"),
      Row("3", "10d:30:00 AM", "10", "1", "1"),
      Row("4", "10d:30:00 AM", "10", "1", "1"),
      Row("5", "10d:30:00 AM", "10", "1", "2"),
      Row("6", "10d:30:00 AM", "10", "1", "2")))

  test("Basic contribution bounding is applied to filter a DataFrame and limit records" +
    "Test that at most `maxContributions` occurrences of Day 1 records are present in `dataFrame`") {
    val rdd: RDD[Row] = spark.sparkContext.parallelize(visitors.data)
    val dataFrame = spark.createDataFrame(rdd, visitors.schema)
    val maxContributions = 3

    val dayContributions = dataFrame
      .transform(BoundContribution("Day", maxContributions))
      .where("Day = 1")
      .count.toInt
    assert(dayContributions === maxContributions)
  }

  test("A private count with contribution bounding can be performed") {
    val rdd: RDD[Row] = spark.sparkContext.parallelize(visitors.data)
    val dataFrame = spark.createDataFrame(rdd, visitors.schema)

    val maxContributions = 3
    val epsilon = 1

    val privateCount = new PrivateCount(epsilon, maxContributions)
    dataFrame.transform(BoundContribution("Day", maxContributions))

    assert(dataFrame.groupBy("Day").agg(privateCount.toColumn.name("cnt")).count.toInt > 0)

    spark.udf.register("privateCount", functions.udaf(privateCount))
    dataFrame.createOrReplaceTempView("test_table")
    assert(spark.sql("SELECT Day, privateCount(VisitorId) as cnt FROM test_table group by Day").count.toInt > 0)
  }
}