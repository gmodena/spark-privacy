/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package privacy.spark

import org.apache.spark.sql.{DataFrame, Encoder, Encoders}
import com.google.privacy.differentialprivacy.Count
import org.apache.spark.sql.functions.{col, row_number}
import org.apache.spark.sql.expressions.{Aggregator, Window}

class PrivateCount(epsilon: Double, contribution: Int) extends Aggregator[Long, Count, Long] {
  override def zero: Count = Count.builder()
    .epsilon(epsilon)
    .maxPartitionsContributed(contribution)
    .build()

  override def reduce(b: Count, a: Long): Count = {
    b.increment()
    b
  }

  override def merge(b1: Count, b2: Count): Count = {
    b1.mergeWith(b2.getSerializableSummary)
    b1
  }

  override def finish(reduction: Count): Long = reduction.computeResult().toLong

  override def bufferEncoder: Encoder[Count] = Encoders.kryo[Count]

  override def outputEncoder: Encoder[Long] = Encoders.scalaLong
}


object BoundContribution {
  def apply(key: String, contributions: Int)(dataFrame: DataFrame): DataFrame =  {
    val byCol = Window.partitionBy(key)

    val tmpCol = "boundCount"
    dataFrame
      .withColumn(tmpCol, row_number over byCol.orderBy(key))
      .where(col(tmpCol) <= contributions)
      .drop(tmpCol)
  }
}

